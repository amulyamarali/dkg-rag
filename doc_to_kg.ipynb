{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ashwini akka will modify this ðŸ¥³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Initialize SpaCy for sentence segmentation and dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the REBEL model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# KB class to manage relations\n",
    "class KB:\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations if self.are_relations_equal(r1, r)][0]\n",
    "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"] if span not in r2[\"meta\"][\"spans\"]]\n",
    "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "# Extract relations using SpaCy dependency parsing\n",
    "def extract_spacy_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":  # The main verb in the sentence\n",
    "            subject = [w for w in token.lefts if w.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "            obj = [w for w in token.rights if w.dep_ in (\"dobj\", \"attr\", \"prep\", \"pobj\")]\n",
    "            if subject and obj:\n",
    "                relations.append({\n",
    "                    'head': subject[0].text,\n",
    "                    'type': token.text,  # The verb as the relation type\n",
    "                    'tail': obj[0].text,\n",
    "                    'meta': {'sentence': text, 'spans': []}\n",
    "                })\n",
    "    return relations\n",
    "\n",
    "# Extract relations from Rebel model output\n",
    "def extract_relations_from_model_output(text, sentence):\n",
    "    relations = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation and subject and object_:\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip(),\n",
    "                    'meta': {'sentence': sentence, 'spans': []}\n",
    "                })\n",
    "                relation, subject, object_ = '', '', ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "\n",
    "    if subject and relation and object_:\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip(),\n",
    "            'meta': {'sentence': sentence, 'spans': []}\n",
    "        })\n",
    "    return relations\n",
    "\n",
    "# Unified function to extract triples using both SpaCy and Rebel\n",
    "def from_text_to_kb(text, kb, span_length=64):\n",
    "    sentences = [sentence.strip() for sentence in text.split(\".\") if sentence.strip()]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Extract relations using SpaCy\n",
    "        spacy_relations = extract_spacy_relations(sentence)\n",
    "        for relation in spacy_relations:\n",
    "            kb.add_relation(relation)\n",
    "\n",
    "        # Extract relations using Rebel\n",
    "        inputs = tokenizer([sentence], return_tensors=\"pt\")\n",
    "        num_tokens = len(inputs[\"input_ids\"][0])\n",
    "        num_spans = math.ceil(num_tokens / span_length)\n",
    "        overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))\n",
    "        spans_boundaries = []\n",
    "        start = 0\n",
    "        for i in range(num_spans):\n",
    "            spans_boundaries.append([start + span_length * i, start + span_length * (i + 1)])\n",
    "            start -= overlap\n",
    "\n",
    "        tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]] for boundary in spans_boundaries]\n",
    "        tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]] for boundary in spans_boundaries]\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.stack(tensor_ids).to(device),\n",
    "            \"attention_mask\": torch.stack(tensor_masks).to(device)\n",
    "        }\n",
    "\n",
    "        num_return_sequences = 5\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": 512,\n",
    "            \"length_penalty\": 1.0,\n",
    "            \"num_beams\": 5,\n",
    "            \"num_return_sequences\": num_return_sequences\n",
    "        }\n",
    "\n",
    "        generated_tokens = model.generate(**inputs, **gen_kwargs)\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        for i, sentence_pred in enumerate(decoded_preds):\n",
    "            current_span_index = i // num_return_sequences\n",
    "            relations = extract_relations_from_model_output(sentence_pred, sentence)\n",
    "            for relation in relations:\n",
    "                relation[\"meta\"][\"spans\"] = [spans_boundaries[current_span_index]]\n",
    "                kb.add_relation(relation)\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\")\n",
    "\n",
    "# Prepare the dataset (for 20 rows)\n",
    "def prepare_dataset(dataset):\n",
    "    dataset = dataset.select(range(100))  # Select only the first 20 rows\n",
    "    dataset = dataset.map(lambda x: {\n",
    "        'user_input': x['question'],\n",
    "        'reference': x['answers']['text'][0] if x['answers']['text'] else '',\n",
    "        'response': '',  # Placeholder for the generated response\n",
    "        'retrieved_contexts': [x['context']]  # Make sure this is a list of contexts\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "prepared_dataset = prepare_dataset(dataset[\"train\"])\n",
    "\n",
    "# 2. **Initialize the Embedding Model**\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 3. **Create a FAISS Index from the Dataset's Contexts**\n",
    "def create_faiss_index(dataset, embedding_model):\n",
    "    contexts = [example['retrieved_contexts'][0] for example in dataset]\n",
    "    context_embeddings = embedding_model.encode(contexts, convert_to_numpy=True)\n",
    "\n",
    "    # Convert context embeddings to numpy array and create the FAISS index\n",
    "    context_embeddings = context_embeddings.astype(np.float32)\n",
    "\n",
    "    # Initialize FAISS index\n",
    "    index = faiss.IndexFlatL2(context_embeddings.shape[1])  # L2 distance metric\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(context_embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "# Create FAISS index from the dataset\n",
    "faiss_index = create_faiss_index(prepared_dataset, embedding_model)\n",
    "\n",
    "# 4. **Retrieve Relevant Contexts from FAISS**\n",
    "def retrieve_relevant_context(query, faiss_index, embedding_model, top_k=1):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding.astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    # Perform a similarity search (retrieving top_k nearest contexts)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "\n",
    "    return indices\n",
    "\n",
    "# Initialize distilbart model and tokenizer for local inference\n",
    "distilbart_model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# 5. **Generate Responses with Retrieved Context**\n",
    "def generate_rag_responses(dataset, faiss_index, distilbart_model, tokenizer, embedding_model):\n",
    "    responses = []\n",
    "    for example in tqdm(dataset):\n",
    "        user_input = example['user_input']\n",
    "\n",
    "        # Retrieve the most relevant context(s)\n",
    "        relevant_indices = retrieve_relevant_context(user_input, faiss_index, embedding_model)\n",
    "        relevant_contexts = [dataset[int(idx)]['retrieved_contexts'][0] for idx in relevant_indices[0]]  # Convert idx to int\n",
    "\n",
    "        # Combine the user question and retrieved context for distilbart input\n",
    "        combined_input = f\"Question: {user_input}\\nContext: {'. '.join(relevant_contexts)}\"\n",
    "\n",
    "        # Encode the input and generate a response\n",
    "        inputs = tokenizer(combined_input, return_tensors=\"pt\", truncation=True)\n",
    "        outputs = distilbart_model.generate(**inputs)\n",
    "\n",
    "        # Decode the generated output\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Generate responses and add them to the 'response' column\n",
    "responses = generate_rag_responses(prepared_dataset, faiss_index, distilbart_model, tokenizer, embedding_model)\n",
    "\n",
    "# Fill the 'response' column with generated responses\n",
    "for idx, response in enumerate(responses):\n",
    "    prepared_dataset[idx]['response'] = response\n",
    "\n",
    "# Print the first row to check the response\n",
    "print(prepared_dataset[0])  # Print the first row to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET PRE-PROCESSING\n",
    "\n",
    "contexts = list()\n",
    "questions = list()\n",
    "answers = list()\n",
    "count = 0\n",
    "\n",
    "for entry in prepared_dataset:\n",
    "  con = entry[\"context\"]\n",
    "  que = entry[\"question\"]\n",
    "  ans = entry[\"answers\"][\"text\"]\n",
    "\n",
    "# context already extracted\n",
    "  if con in contexts:\n",
    "    que_list = questions[count - 1]\n",
    "    que_list.append(que)\n",
    "    questions[count - 1] = que_list\n",
    "\n",
    "    ans_list = answers[count - 1]\n",
    "    ans_list.append(ans)\n",
    "    answers[count - 1] = ans_list\n",
    "\n",
    "# new context\n",
    "  else:\n",
    "    contexts.append(con)\n",
    "    count += 1\n",
    "    q_list = list()\n",
    "    q_list.append(que)\n",
    "\n",
    "    a_list = list()\n",
    "    a_list.append(ans)\n",
    "\n",
    "    questions.append(q_list)\n",
    "    answers.append(a_list)\n",
    "\n",
    "\n",
    "for i in range(len(contexts)):\n",
    "\n",
    "  print(f\"context {i+1} : {contexts[i]}\")\n",
    "\n",
    "  for j in range(len(questions[i])):\n",
    "    print(f\"question {j+1} : {questions[i][j]}\")\n",
    "    print(f\"answer : {answers[i][j]}\")\n",
    "\n",
    "  print(\"\\n\")\n",
    "\n",
    "kb = KB()\n",
    "\n",
    "for cont in contexts:\n",
    "  kb = from_text_to_kb(cont, kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to export relations to a CSV file\n",
    "\n",
    "\n",
    "def export_relations_to_csv(kb, output_file):\n",
    "    # Define the CSV headers\n",
    "    headers = [\"Sentence\", \"Head\", \"Relation\", \"Tail\"]\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)  # Write the headers\n",
    "\n",
    "        # Write each relation in the KB\n",
    "        for relation in kb.relations:\n",
    "            writer.writerow([\n",
    "                relation['meta']['sentence'],  # Sentence\n",
    "                relation['head'],              # Head\n",
    "                relation['type'],              # Relation\n",
    "                relation['tail']               # Tail\n",
    "            ])\n",
    "\n",
    "    print(f\"Relations successfully exported to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "kb = KB()\n",
    "# Assume you already populated the KB using from_text_to_kb\n",
    "from_text_to_kb(\"John wrote a book. Alice loves Bob.\", kb)\n",
    "\n",
    "# Export to a CSV file\n",
    "export_relations_to_csv(kb, \"relations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
